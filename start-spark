#!/usr/bin/env bash

# Set default HDFS user if not set
if [[ -z "${HDFS_USER}" ]]; then
  export HDFS_USER=spark
fi
master_host=$(echo $2 | cut -d":" -f 1 )
master_port=$(echo $2 | cut -d":" -f 2 )
if [[ "${1}" = 'master' ]]; then
  # Start Hadoop NameNode
  start-hadoop namenode daemon
  # Start Spark Master
  spark-class org.apache.spark.deploy.master.Master -h $master_host --port $master_port
elif [[ "${1}" = 'worker' ]]; then
  # Start Hadoop DataNode
  start-hadoop datanode $2 daemon
  # Wait for the master to start
  while ! nc -z $master_host $master_port; do
    sleep 2;
  done;
  # Start Spark Worker
  spark-class org.apache.spark.deploy.worker.Worker spark://$2
else
  echo "Invalid command '${1}'" >&2
  exit 1
fi
